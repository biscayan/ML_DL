# 1. 신경망 입문

## 1.2 신경망의 기본 구조
신경망은 크게 **단층 신경망**과 **다층 신경망**으로 나눠볼 수 있다.  
단층 신경망에서는 일단의 입력들이 선형 함수를 일반화한 함수를 통해서 하나의 출력에 직접 연결되어 있다. 이러한 단순한 신경망 구조를 **퍼셉트론**이라고 한다.  
다층 신경망에는 뉴런들이 여러 층으로 배치되는데, 입력층과 출력층 사이에 하나 이상의 은닉층들이 있다. 이러한 층별 신경망 구조를 **순방향 신경망**이라고 한다.  

## 1.2.1 단일 계산층: 퍼센트론
가장 단순한 형태의 신경망을 퍼셉트론이라고 한다. 이 신경망은 하나의 입력층과 하나의 출력 노드로 구성된다.  
입력층은 *d*개의 노드로 구성되며, *d*개의 특징들로 이루어진 벡터의 가중치들이 간선들을 통해 출력 노드에 전달된다. 입력층 자체는 그 어떤 계산도 하지 않으며, 선형 함수는 출력 노드에서 계산된다. 이때, 입력층은 그 어떤 계산도 수행하지 않고 그냥 특징 값들을 전달하기만 하기 때문에 신경망의 층수(깊이)를 셀 때 입력층은 포함하지 않는다.  
  
퍼셉트론의 입력층은 특징들을 출력 노드에 전달한다. 입력에서 출력으로의 간선들에는 가중치들이 부여되어 있다. 퍼셉트론은 입력된 특징들에 가중치들을 곱해서 모두 합한 후, 그 합에 부호 함수를 적용해서 최종적인 분류명을 산출한다. 이 부호 함수는 **활성화 함수**의 역할을 한다. 활성화 함수를 어떻게 선택하느냐에 따라 기계 학습에 쓰여온 여러 종류의 기존 모형들을 흉내 낼 수 있다. 예를들어, 수치 목푯값 최소제곱 회귀 모형이나 지지벡터기계(SVM), 로지스틱 회귀 분류기를 흉내 내는 것이 가능하다. 대부분의 기초적인 기계 학습 모형들은 단순한 신경망 구조로 손쉽게 표현할 수 있다.

기본적인 퍼셉트론 알고리즘은 확률적 경사 하강법에 해당한다. 확률적 경사 하강법은 무작위로 선택된 훈련점들에 경사 하강법을 적용해서 예측값의 제곱오차를 암묵적으로 최소화한다.  
미니배치 확률적 경사 하강법은 무작위로 선택한 훈련 자료 부분집합의 훈련점들에 경사하강법을 적용한다.  

### 1.2.1.1 퍼셉트론이 최적화하는 목적함수
기울기가 퍼셉트론 갱신에 해당하는 매끄러운 손실함수가 있을까? 이진 분류 문제에서 오분류 횟수는 다음과 같이 훈련 자료점에 대한 0/1 손실함수의 형태로 표현할 수 있다. 하지만, 이 목적함수는 계단 형태라서 미분 가능 함수가 아니다. 신경망은 기울기 기반 최적화에 의존하므로, 퍼셉트론 갱신에 적합한 매끄러운 목적함수를 정의할 필요가 있다.  
미분 불가능 함수의 기울기 계산이 가능하도록 수정한 손실함수를 평활화된 대리 손실함수라고 한다. 출력이 이산값인 연속 최적화 기반 학습 방법들은 거의 대부분 이런 평활화된 대리 손실함수를 사용한다.  

퍼셉트론의 갱신이 반복되면서 손실값이 개선되긴 하지만 오히려 오분류 횟수가 크게 느는 현상도 발생할 수 있다. 이는 대리 손실함수가 원래 의도한 목표를 완전히 달성하지는 못할 수 있기 때문이다. 이 때문에, 대리 손실함수 접근 방식은 안정적이지 못하다. 최종적인 해의 품질이 크게 들쭉날쭉할 수 있다. 그래서 학습 알고리즘을 분리가능이 아닌 자료를 위해 변형한 버전들이 다양하게 제안되었다.  
한 가지 자연스러운 접근 방식은 오분류 횟수 면에서 지금까지 최상의 해를 기억해 두는 것이며 이러한 방식을 주머니 알고리즘이라고 한다.  
성과가 아주 좋은 또 다른 변형으로는 손실함수에 여유(margin)라는 개념을 도입한 알고리즘도 있다. 그런 식으로 변형한 알고리즘은 선형 지지 벡터 기계의 알고리즘과 사실상 동일하다. 이 때문에 선형 지지 벡터 기계를 최적 안전성 퍼셉트론이라고 부르기도 한다. 

### 1.2.1.2 퍼셉트론 판정기준과 지지 벡터 기계의 관계
퍼셉트론 판정기준은 지지 벡터 기계에 쓰이는 경첩 손실 함수를 한쪽으로 이동한 버전에 해당한다. 퍼셉트론과 지지 벡터 기계는 기원은 서로 다르지만 근본적으로 많이 다르지는 않다. 프로인트와 섀피어는 손실함수에 도입된 여유 개념이 퍼셉트론의 안정성 개선에서, 그리고 지지 벡터 기계와의 관계에서 어떤 역할을 하는지를 훌륭하게 설명했으며 다수의 전통적인 기계 학습 모형들을 퍼셉트론 같은 얕은 신경망 구조의 사소한 변형으로 해석할 수 있음이 밝혀졌다.

### 1.2.1.3 활성화 함수와 손실함수의 선택
